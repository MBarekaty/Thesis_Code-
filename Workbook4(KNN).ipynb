{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing processing libraries and working path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspace path \n",
    "os.chdir(r\"C:\\Thesis_data\\Data\\working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing setup assessment and initial data\n",
    "##### Processing Parameters  \n",
    "Changing Parameters from TRUE to FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vegetation_indices=True\n",
    "use_MinMaxScaler=True\n",
    "use_hyperparameter_tuning =False\n",
    "use_oversampling= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('Train_split_70_p121.CSV')\n",
    "print('We have {} train data with {} variables'.format(*train_features.shape))\n",
    "test_features = pd.read_csv('Test_split_30_p121.CSV')\n",
    "print('We have {} test data with {} variables'.format(*test_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "<blockquote><b>KNN</b><br>K nearest neighbours processing steps starts with predicting and finding the accuracy and trying different options such as <b>using oversampling</b>, using <b>hyperparameter tuning</b> and <b>Vegetation indices</b> the result of each option has been provided below</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extracting value for test and train dataset for randomforest assessment</b><br>\n",
    "<b>Training process without considering different indices</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vegetation_indices == True:\n",
    "    # included vegetation indices\n",
    "    X_train = train_features.iloc[:,2:]\n",
    "    y_train = train_features.iloc[:,0]\n",
    "    X_test = test_features.iloc[:,2:]\n",
    "    y_test = test_features.iloc[:,0]\n",
    "    \n",
    "else:\n",
    "    # Included DSM\n",
    "    X_train = train_features.iloc[:,2:6]\n",
    "    y_train = train_features.iloc[:,0]\n",
    "    X_test = test_features.iloc[:,2:6]\n",
    "    y_test = test_features.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_MinMaxScaler==True:\n",
    "    trans = MinMaxScaler()\n",
    "    X_train = trans.fit_transform(X_train)\n",
    "    X_test = trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_oversampling == True:\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "    print('Resampled train dataset shape %s' % Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_hyperparameter_tuning == True:\n",
    "    grid_params={\"n_neighbors\":[9,11,19,35,51],\"weights\":[\"uniform\",\"distance\"],\"metric\":[\"euclidean\",\"manhattan\"]}           \n",
    "    kmodel=GridSearchCV(KNeighborsClassifier(),grid_params,verbose=1,cv=5,n_jobs=-1)\n",
    "    kmodel.fit(X_train,y_train)\n",
    "    y_pred=kmodel.predict(X_test)\n",
    "    scores_df = pd.DataFrame(kmodel.cv_results_)#scores_df = pd.DataFrame(grid_rf.cv_results_)\n",
    "    print(\"Best paramters:\\n\", kmodel.best_params_) #best=grid_rf.best_params_\n",
    "    print(\"Best accuracy scores:\", kmodel.best_score_)\n",
    "    #Prediction\n",
    "    prediction=kmodel.predict(X_test)\n",
    "    print(\"Accuracy score based on test data with Hyperparameter tuning\",round(metrics.accuracy_score(prediction,y_test)*100,2), '%.')\n",
    "    print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "    print(classification_report(y_test, prediction))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(kmodel, X_test, y_test ,  ax=ax,cmap=plt.cm.YlGnBu)\n",
    "    plt.title('Confusion matrix KNN hyperparameter model')\n",
    "    plt.savefig(\"figure4.png\") \n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "else:\n",
    "    kmodel=KNeighborsClassifier()\n",
    "    kmodel=kmodel.fit(X_train,y_train)\n",
    "    y_pred_knn=kmodel.predict(X_test)\n",
    "    print(\"Accuracy for KNN base model :\",round(metrics.accuracy_score(y_pred_knn,y_test)*100,2), '%.')\n",
    "    #evaluation(Confusion Metrix)\n",
    "    cm=metrics.confusion_matrix(y_pred_knn,y_test)\n",
    "    print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(y_pred_knn,y_test))\n",
    "    print(classification_report(y_pred_knn,y_test))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(kmodel, X_test, y_test ,  ax=ax,cmap=plt.cm.YlGnBu)\n",
    "    plt.title('Confusion matrix KNN base model')\n",
    "    plt.savefig(\"figure5.png\") \n",
    "    plt.show()\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of True and False predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_features.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data=y_pred_knn)\n",
    "df1.rename(columns={0: 'Predicted Labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = pd.concat([df, df1],axis = 1)\n",
    "#path=r\"C:\\Thesis_data\\Data\\working\"\n",
    "#Mtrainp11 = os.path.join(path,'Mtrainp11.csv')\n",
    "#df_col.to_csv(Mtrainp11, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = np.where(df_col['Predicted Labels'] == df_col['Class'], 'True', 'False')\n",
    "df_compare=pd.DataFrame(data=df_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df_compare, df_col],axis = 1)\n",
    "df3.rename(columns={0: 'Compared Labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['Compared Labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
