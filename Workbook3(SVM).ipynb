{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from rasterio.plot import show\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspace path\n",
    "os.chdir(r\"C:\\Thesis_data\\Data\\working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing setup assessment and initial data \n",
    "##### Processing Parameters  \n",
    "Changing Parameters from TRUE to FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vegetation_indices = True\n",
    "use_MinMaxScaler= True\n",
    "use_hyperparameter_tuning = True\n",
    "use_oversampling =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('Train_split_70_p121.CSV')\n",
    "print('We have {} train data with {} variables'.format(*train_features.shape))\n",
    "test_features = pd.read_csv('Test_split_30_p121.CSV')\n",
    "print('We have {} test data with {} variables'.format(*test_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "<blockquote><b>SVM : </b><br>Support Vector Machines steps it starts with predicting and finding the accuracy and trying different options such as <b>using oversampling</b>, using <b>hyperparameter tuning</b> and <b>Vegetation indices</b> the result of each option has been provided below</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extracting value for test and train dataset for SVM assessment</b><br>\n",
    "<b>Training process without considering different indices</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vegetation_indices == True:\n",
    "    # included vegetation indices\n",
    "    X_train = train_features.iloc[:,2:]\n",
    "    y_train = train_features.iloc[:,0]\n",
    "    X_test = test_features.iloc[:,2:]\n",
    "    y_test = test_features.iloc[:,0]\n",
    "\n",
    "else:\n",
    "    # Included DSM\n",
    "    X_train = train_features.iloc[:,2:6]\n",
    "    y_train = train_features.iloc[:,0]\n",
    "    X_test = test_features.iloc[:,2:6]\n",
    "    y_test = test_features.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_MinMaxScaler==True:\n",
    "    trans = MinMaxScaler()\n",
    "    X_train = trans.fit_transform(X_train)\n",
    "    X_test = trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_oversampling == True:\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "    print('Resampled train dataset shape %s' % Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if use_hyperparameter_tuning == True:\n",
    "    svm = SVC(probability = True)\n",
    "    parameters ={\"kernel\":['rbf'], 'C': [1,10],'gamma' : [0.001,0.1], 'degree':[1, 2]}\n",
    "    grid_svm = GridSearchCV(svm, param_grid = parameters, cv = 3, n_jobs = -1)\n",
    "    grid_svm.fit(X_train, y_train)\n",
    "    scores_df = pd.DataFrame(grid_svm.cv_results_)\n",
    "    print(\"Best paramters:\", grid_svm.best_params_)\n",
    "    print(\"Best accuracy scores:\", grid_svm.best_score_)\n",
    "    pred = grid_svm.predict(X_test)\n",
    "    print(\"Accuracy for SVM hyperparameter model :\",round(metrics.accuracy_score(pred,y_test)*100,2), '%.')\n",
    "    print(confusion_matrix(y_test,pred))\n",
    "    print(classification_report(y_test,pred))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(grid_svm, X_test, y_test ,  ax=ax,cmap=plt.cm.YlGnBu)\n",
    "    plt.title('Confusion matrix SVM hyperparameter model')\n",
    "    plt.savefig(\"figure3.png\") \n",
    "    plt.show()\n",
    "else:\n",
    "    \n",
    "    from sklearn import svm\n",
    "    model=svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', verbose=False,random_state=None)\n",
    "    #learning\n",
    "    model.fit(X_train,y_train)\n",
    "    #Prediction\n",
    "    prediction=model.predict(X_test)\n",
    "    #evaluation(Accuracy)\n",
    "    print(\"Accuracy for SVM base model :\",round(metrics.accuracy_score(prediction,y_test)*100,2), '%.')\n",
    "    #evaluation(Confusion Metrix)\n",
    "    cm=metrics.confusion_matrix(prediction,y_test)\n",
    "    print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "    print(classification_report(y_test,prediction))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(model, X_test, y_test ,  ax=ax,cmap=plt.cm.YlGnBu)\n",
    "    plt.title('Confusion matrix SVM base model')\n",
    "    plt.savefig(\"figure.png\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of True and False predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_features.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data=prediction)\n",
    "df1.rename(columns={0: 'Predicted Labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = pd.concat([df, df1],axis = 1)\n",
    "##path=r\"C:\\Thesis_data\\Data\\working\"\n",
    "#Halfp11 = os.path.join(path,'Halfp11.csv')\n",
    "#df_col.to_csv(Halfp11, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = np.where(df_col['Predicted Labels'] == df_col['Class'], 'True', 'False')\n",
    "df_compare=pd.DataFrame(data=df_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df_compare, df_col],axis = 1)\n",
    "df3.rename(columns={0: 'Compared Labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['Compared Labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
